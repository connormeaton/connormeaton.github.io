---
title: 'Alternatives to Selling Out'
date: 2020-10-12
permalink: /posts/2020/10/alternatives-to-selling-out/
tags:
  -
---

### TLDR;
 
Only when we are free to deconstruct our constructions can we morally pursue technological progression.

### AM I A SELLOUT?

Recently, a good friend called me a sellout for my choice of employment and for some time before this, my wife had been gently prodding at my profession intentions. While jarring, I’ve learned that direct and honest criticism from those who want the best for you is of immeasurable value. As I marinated in their comments, longstanding jaw tension and stomach pangs catalyzed into concrete concerns. They pointed out something that already bothered me subconsciously. For better or worse, I now have to contend with these questions. 

To clarify, I have ‘sold out’ because I have willfully chosen to earn an income building emotionally perceptive AI, a technology whose societal gains I do not believe, at this moment in 2020, to be worth the risks. While I do not know if this is constitutes “selling out”, their words exposed a moral mess and its ugliness demands sorting. In doing so, I will crudely map the space of the technology in discussion, articulate reasons for my involvement, and finally, consider my personal responsibility. Hopefully this exercise will not be context specific, and that its threads can be woven through any of the morally ambiguous cloths that create the fabric of our society.

### NOTE ON LANGUAGE

Before we begin, I want to make clear when I use the term ‘emotionally-perceptive AI’ (EAI), I do not actually mean it. I do not think it will be possible for machines to read people’s emotions in the same way I do not think people can read people’s emotions, including their own. Instead, I am focusing on training machines to perceive discrete behavioral patterns that exhibit enough objective similarities to be formed into categories. It pains me to use imprecise language, but I will use EAI until a better way to communicate the concept is discovered.

### GOODS < BADS

From my perspective, which regrettably encapsulates the optimistic romantic full of ideals, the pessimistic nihilist tainted with cynicism, the scientist fascinated with understanding new things, and the luddite intent on smashing things that haven’t proven themselves desirable, it looks like EAI’s bads outweigh the goods. To schematically understand this, see the ‘EAI Quadrants of Influence’ figure below.

[![Picture1.png](https://i.postimg.cc/0NBt0xGC/Picture1.png)](https://postimg.cc/gw3qGCTw)
	 
In this figure, you will find four quadrants on an x-y plane. The x-axis represents the overall goodness towards society, and the y-axis indicates the likelihood of that outcome occurring. Each quadrant is filled with a hypothetical scenario(s) according to its perceived likelihood and goodness.
	
Quadrant 1: Quadrant 1 kicks off the table with the least likely, least good, and most scary hypothetical outcome, the incorporation of EAI into state surveillance. At this time, I do not know the status of state surveillance and, I’m unsure how to trust sources on the subject. However, I am certain that it is entirely possible, which is reason enough for concern. High resolution cameras are cheap, face recognition algorithms are fairly accurate, and we willingly submit text, audio, video, and GPS data into connected systems. None of this should be alarming as sophisticated inference regarding our behavior is already possible. The revolutionary aspect of EAI however, is that it provides a direct signal to our behavior-state straight from our face, posture, vocal tone, heart rate, spoken words, etc. Already exploited proxies such as browsing history or Instagram hearts feel intrusive, but they pale to the direct harvest of multimodal bodily signals.

From my civilian perspective, armed with several readings of 1984, an obsession with 20th century authoritarian governments, and the fact that I came of age in the post-Snowden era, I don’t like state surveillance. It feels antagonistic to liberty, which I believe to be the cornerstone of a healthy society. On the other hand, I am rather naïve to state security matters and my assignment of ‘all bad’ to this topic is unfounded. I can’t help but wonder if the EAI and surveillance fusion could have stopped the Sandy Hook Massacre or the September 11th catastrophes. In such tragic cases, I’m sure some perceivable signal, such as facial expressions or physiological cues, could have forecast enough suspicion to warrant preemptive action. It would have been better if those events did not happen, but is cashing in our constant behavior/emotion-state to a central agency to pay the price worth it? Maybe. I do not pretend to know. 

Quadrant 2: Quadrant 2 is much lighter, occupying the least likely and most good region of our outcome space. The deployment of EAI in consumer applications could be revolutionary for relationship health. Typically, I am an advocate for the simplest solution to a problem, and I firmly believe that the hammer of technology is not always the best tool to drive a fastener, particularity when we are dealing with screws. However, people can be really terrible. We can be mean, selfish, evasive, and even when we are trying to support our loved ones, we can and do make them cry. The romantic in me wants to place my hopes in more social and analogue solutions, like meditation, religious teachings, and honest conversation, but we’ve had these tools for thousands of years and terrible relationships are still normal. The slow destigmatizing of therapy is promising, but I hesitate to bet on a system that requires tens of thousands of dollars in graduate tuition and minimal compensation during a lengthy and excessively bureaucratic licensure process to provide a service that is expensive and hard on every party involved. If improving relationships truly is the goal, then I think we have to turn to technology, to EAI.

Imagine a world where a smart phone app monitors your behavioral output, much like a Fitbit already does for physiological signals. If we sidestep creepy, which I acknowledge may be difficult, the benefits here are endless. For instance, image you are issuing unsolicited advice to your children at the dinner table (again). Usually, after 45 seconds, your begrudging wife pinches your thigh to inform you that you’re domineering the conversation, something she hates doing. Instead, your EAI app could have been tracking your conversational output and began vibrating after 10 consecutive seconds of predicted domineering, alerting you before your wife even notices. Assuming you listen and course correct, your kids may feel more respected and your wife may be liberated to ruminate on your more admirable qualities. Over time, the app could train the out-of-context domineering out of you and eventually be less and less necessary, like a dog wearing an electric fence collar. This would lead to healthier family dinners which in turn cultivate stronger families, the foundational social unit of all cultures. 

Quadrant 3: Quadrant 3 is a doozy, containing hypothetical outcomes that are both likely and bad. I illustrated 3 examples below.

-	More manipulative advertising techniques: Thanks to the data collection machines that are Google, Facebook, etc., advertising is already very persuasive, intrusive, disturbing, and a waste of innovation. One could argue that it is preferable to be inundated with tailored ads instead of girthy salesmen yelling about discount furniture on cable television. However, if it can be determined that you are experiencing depressive symptoms, it could reliably be assumed that you have a hole that buying stuff could fill. This would allow advertising agencies to place ads with increasing precision, bombarding you when you are sad and easing off when you are more stable and in less need of material distraction. Depending on your online presence, this is already happening. However, it is reasonable to assume that EAI will increase the intensity.

-	Hiring/employment requirements: EAI will likely make its way into hiring and employee monitoring practices. For example, your interview process (as well as your entire internet presence), could be analyzed for behavioral patterns. Particularly in saturated markets where application filters are often utilized, this would provide another feature to discriminate between candidates. For the hiring manager burdened with thousands of applications, automatically slicing the pile by removing all candidates exhibiting contempt in more than 2% of their interactions would be very attractive. Concerning the already employed, more and more meetings are held over video and could be easily analyzed with enterprise EAI software. As a result, job security could be contingent upon a new metric that we are wildly unprepared to use. One could argue this might improve things by, for example, leveling the playing field for more agreeable personalities that often get outcompeted for in the corporate ecosystem. More likely is that such an intrusive and impersonal management tool would breed resentment, dishonesty, and anxiety.

-	Outsourcing of inter-/intra-social critique and responsibility: People are judgmental and critical of others. While this is typically experienced as annoying or mean when expressed, such nitpicking can, and often does, serve the much-needed purpose of socialization. Without others subtlety chiseling away your animal nature, you would be an intolerable monster. If EAI apps become more popular, a bystander effect will likely be created. Confrontation is uncomfortable, so if we can assume that an EAI app will guide someone else’s relational development, then we don’t have to get our hands dirty. Maybe the apps will do better than our pokes and prods, but using their existence as an excuse to avoid conflict would lead to superficial, stale, and inauthentic relationships.

Quadrant 4: Hooray! We made it to quadrant 4, the hypothetical space that is both likely and inarguably good. Mental health care is a good thing and thanks to the work of visionary therapists and brave clients, it is becoming more normalized in our society. However, significant barriers to mental health care still remain, including low supply, high expense, and the difficulty of objectively measuring outcomes. Using EAI, mental health care workers could have more precise diagnostic tools, the opportunity to extend care beyond clinical sessions, and empirical confidence in their treatment plans. As a result, mental health care could be cheaper, easier, more effective, and eventually, less stigmatized. I cannot see a downside to this.

While the quadrants are hypothetical, uncertain, and incomplete, they are reasonable. Quadrant 1 could turn democracy into an illusion. Quadrant 2 could lay the strongest and healthiest societal foundations we’ve ever known. Quadrant 3 could be kind of like now but worse. Quadrant 4 could make mental health care way better and more accessible. If 1 and 3 were placed on the left and 2 and 4 on the right side of a grand scale weighing long-term social good, I do not know which pairing would tilt the balance. What does seem likely however, is that reaping the positive benefits will require direct and difficult action, thus limiting its reach, whereas everyone will be affected by the negatives without consent or effort. While I believe in the seemingly infinite positive potential of healthy human relationships, a negative bias is more convincing. I would love to be convinced otherwise and hope I am wrong.

### WHY?

If I believe the technology to be net negative, even if only slightly so, why do I spend so much of my life working on it? Briefly, I got into this niche space for ethical concerns. I saw the field as a train barreling towards doom with no one to course correct and I wanted to do something about it. This was naïve. As we discussed, it may be reckless but it’s not all bad. Besides, the amount of funding and people smarter than me working on this implies that course correction by one relatively obscure scientist is unlikely, assuming there are better alternative tracks to steer onto (unclear). 
	
Over time, this initial drive for involvement faded and was replaced with pure scientific interest. Modeling human behavior and emotion seems to me, the richest way to study human behavior and emotion. Computers have none of our prior knowledge. When we try to teach them things that we assume to be explicit, we discover all sorts of implicit rules that we were previously ignorant of. For example, many of us modern humans use the phrase ‘No fucking way!’ as a term of validating endearment despite the literal interpretation of the words indicating profane disagreement. This is really confusing to a computer. This is just one example, but it illustrates that our mind constructs categories according to rules we don’t fully understand. From my perspective, there aren’t many things more interesting than trying to understand how we know things without knowing how we know them. Amazingly, I have found myself in a place where I can earn money doing just that, which leads me to the less idealist but more concrete reason why I work on this technology.

It pays well. Not competitive with a typical tech salary, but a more my last three jobs, which were teaching general biology as a grad student, vegetable farming, and shoe making. To be honest, I would probably be working on this even if it didn’t compensate as well because it is so interesting, but I want to be careful not to assume material wealth has no sway over me. I may be idealistic, but I bought a car and went on vacation last year, and that was pretty awesome.

The last major reason is that while I have my fears, I do believe in the goods this technology can provide. A world where people have better relationships is a better world. If I could help usher in this (or any) better world, even if by only planting a few seeds or laying a few bricks, that would be a meaningful life. However, as we have already pointed out, this is the romantic idealist talking. There is no certainty that my work will lead us closer to heaven for it could just as easily, if not more easily, lead us closer to hell. Apparently, my unexamined actions indicate that I am content with this. Why? Because it’s interesting, it pays well, and there’s a slight chance it will be good. I am not proud to say that, so what should I do instead?

### THE FUTILITY OF STOPPING

A reasonable solution would be to stop contributing to this field. However, after little analysis, this approach doesn’t appear to be beneficial. The technology will still be developed regardless of my involvement. Sure, I’m fairly good at this and have been able to deliver tangible results with relatively little resources, but I am only one man. There are whole groups of people, all of them smarter and better resourced than me, working on the same problems. The technology will get there, with or without me.
	
One could also argue that change is typically better made from within the system. I do not know how to positively tilt the balance, but I do know that I’m more useful if I know what I am talking about. As the field changes so fast, absenting myself for a few weeks could make my voice irrelevant.
	
Last, is removing myself and subsequently ignoring a problem I don’t have to face yet the right thing to do? I could wash my hands of this all and go back to farming. Nothing feels more honest than feeding my family with a cornucopia of vegetables earned through honest labor. As much as the back-to-the-land mentality is attractive, EAI would still exist and would be just as real as the soil. Any thoughts otherwise would be pretending. Besides, food production in the modern world really isn’t a problem in need of solving anymore. A homesteader could always go to the grocery store whenever they feel like it. Pursuing a goal with such low stakes feels, to me, unnecessary and unrewarding. Maybe I should put more weight on the value of the experience of life, but problems are everywhere, and I really want to solve them. I don’t see any other way to pursue making life maximally better for all, which feels more important than enjoyable experience. This might be ridiculous, I don’t know. It seems true, but something tells me that idea will not age well. I should probably check back I’m no longer in my 20s…

### PERSONAL RESPONSIBILITY

To recap, the prognosis of EAI likely skews bad. I contribute to it because it’s intellectually and financially rewarding and I do whole-heartedly believe in certain applications, but I’m not sure if it’s the right thing to do. I could absent myself from the field. However, it will still progress and I would risk becoming even more irrelevant in plotting a moral course for its growth. So then, what should I do?
	
The main conundrum appears to be the moral ambiguity assigned to the outcomes. For instance, if I believed it skewed good instead of bad, I would have no need to spend so much time writing this. I would just go back work. To circumvent this, I could easily pretend that I think it skews good. People do this all the time, and my life would probably be easier if I did it more. This capacity is a feature of the rational mind, not a bug.

While I want to be cautious that I am not falling into this trap, what if I am wrong about the moral valence of the outcome? It really could skew positive. If I have learned anything while observing the history of scientific advancement, it is that science is gnarly, ambiguous, and very ugly. For example, science made nuclear weapons and very smart people still debate about the net effect of the discovery. Regardless of what side you fall on, the fact that compelling arguments exist on both sides is enough to shatter any logical structure of certainty we may build for the outcome any technology. Existence is so complex and it’s very difficult to sort moral valence in real time.

This might be a rationalization, or it may be the most concrete path I can find, but perhaps I should ascribe less weight to the predictions of my naïvely calibrated forecasting tools. People tinker and make new things, all of which carry the potential to be used for good and bad. This process isn’t something to try to stop because it can’t be stopped. Tinkering is constraint on life that we must deal with in the same way we must deal with the constraint of gravity. Attempting to suppress tinkering only forces it underground where our governing institutions cannot safely monitor or regulate. We tinker by default, but we must choose to contend with the morality of our constructions.

I don’t know what’s going to happen with EAI, none of us do, and that’s okay. This thought-experiment really isn’t even about EAI, but more our personal responsibility in technological progression. Each individual is charged with the responsibility, which are free to reject, of cultivating our shared moral landscape. In doing so, we have a small set of humble tools, reading, writing, and speaking. We have the ability to read about past triumphs and failures, critically adapt that knowledge to the present through writing, and share our ambitions, concerns, and prognosis with others through speaking. This is the only way to create a vibrant and competitive marketplace of ideals to strive for. These tools appear unimpressive, but they literally shape the world. It is because of them that we have not destroyed ourselves in a nuclear apocalypse, even though we’ve been one-button push away from that reality. 

### ATTEMPT AT A PRAGMATIC CONCLUSION

I suppose then, my final prognosis is that I should work on any technological development I find interesting under the conditions that I freely read, write, and speak about its moral implications. As soon as personal or political forces degrade these conditions, I become morally compromised. I am then obligated to absent myself from technological progression and pursue the restoration of the freedom of press, speech, and ultimately, thought. Only when we are free to deconstruct our constructions can we morally pursue technological progression.
